<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pytorch on 冷嘉承</title>
    <link>/tags/pytorch/</link>
    <description>Recent content in pytorch on 冷嘉承</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using pytorch to solve optimization problem</title>
      <link>/post/2021/06/08/using-pytorch-to-solve-optimization-problem/</link>
      <pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021/06/08/using-pytorch-to-solve-optimization-problem/</guid>
      <description>利用pytorch求解最优化问题，可以将最优化问题的目标函数作为Loss进行backward, 所需要求解的最优解设置为YourDeepModel的参数进行更新.
import torch from torch.optim import SGD, Adam, ASGD, LBFGS from torch.nn import MSELoss # define your torch model class yourDeepModel(torch.nn.Module): def __init__(self, args): super(yourDeepModel, self).__init__() self.x = torch.nn.Parameter(torch.rand(x_dims)) pass def forward(self): return # initialization model = yourDeepModel() optimizer = LBFGS(model.parameters(), lr=.01) epoch = 1000 model.train() for i in range(epoch): def closure(): optimizer.zero_grad() optim_value = model() optim_value.backward() return optim_value optimizer.step(closure) if i % 10 == 0: print(f&amp;quot;optim value:{closure().item()}&amp;quot;) # print your trainable variable print(f&amp;quot;optim solution:{model.</description>
    </item>
    
  </channel>
</rss>
